"""
LLM ì¶”ë¡  ëŠ¥ë ¥ í‰ê°€ íŒŒì´í”„ë¼ì¸ (ê°„ì†Œí™” ë²„ì „)

ì£¼ìš” ê¸°ëŠ¥:
1. ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ
2. ì˜ë¬¸ ë²ˆì—­ ë° ì…”í”Œ
3. ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ìƒì„±
4. ë…¸ì´ì¦ˆ ì¡°í•© ë°ì´í„°ì…‹ ìƒì„±
5. í‰ê°€ ìš”ì•½
"""

from dataclasses import dataclass, field
from pathlib import Path
import random
from typing import Dict, List
import warnings

from openai import OpenAI

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")


# ==================== API í´ë¼ì´ì–¸íŠ¸ ====================

client = OpenAI(
    api_key="up_ZDvIwLQKhlVuIrSdimyXmwdFwtSxc", base_url="https://api.upstage.ai/v1"
)


def call_llm(messages: List[Dict], temperature: float = 0.4) -> str:
    """LLM API í˜¸ì¶œ"""
    response = client.chat.completions.create(
        model="upstage/solar-1-mini-chat",
        messages=messages,
        temperature=temperature,
        max_tokens=16384,
        stream=False,
    )
    return response.choices[0].message.content


# ==================== ì„¤ì • ====================


@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""

    # í•„ìˆ˜ ì„¤ì •
    base_context_file: str
    output_dir: str = "./output"

    # ìƒì„± ì„¤ì •
    num_unrelated_contexts: int = 70
    random_seed: int = 42

    # ë‚œì´ë„ ì„¤ì •
    difficulties: Dict = field(
        default_factory=lambda: {
            "easy": {
                "noise_types": ["unrelated", "english"],
                "positions": ["append"],
                "description": "ì ì€ ì–‘ì˜ ë¬´ê´€í•œ ì •ë³´ë¥¼ ëì— ì¶”ê°€",
            },
            "medium": {
                "noise_types": ["unrelated", "english", "similar"],
                "positions": ["prepend", "append", "sandwich"],
                "description": "ìœ ì‚¬ ë„ë©”ì¸ ì •ë³´ë¥¼ ë‹¤ì–‘í•œ ìœ„ì¹˜ì— ì¶”ê°€",
            },
            "hard": {
                "noise_types": ["unrelated", "english", "similar", "contradictory"],
                "positions": ["sandwich", "interleave"],
                "description": "ëª¨ìˆœ ì •ë³´ë¥¼ ë³¸ë¬¸ ì‚¬ì´ì— ì‚½ì…",
            },
            "extreme": {
                "noise_types": [
                    "unrelated",
                    "english",
                    "similar",
                    "contradictory",
                    "partial",
                ],
                "positions": ["interleave", "random"],
                "description": "ë‹¤ì–‘í•œ í˜¼ë€ ì •ë³´ë¥¼ ë¬´ì‘ìœ„ë¡œ ì‚½ì…",
            },
        }
    )


# ==================== ë…¸ì´ì¦ˆ ìƒì„± ====================


def generate_unrelated_contexts(num: int) -> List[str]:
    """ë¬´ê´€í•œ ë²•ë¥  ì¡°í•­ ìƒì„±"""
    print(f"\nğŸ“ ë¬´ê´€í•œ ë²•ë¥  ì¡°í•­ {num}ê°œ ìƒì„± ì¤‘...")

    contexts = []
    for i in range(num):
        print(f"  ì§„í–‰: {i + 1}/{num}", end="\r")

        messages = [
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸ˆìœµ ê´€ë ¨ì´ ì•„ë‹Œ ë²•ë¥ (í—Œë²•, ë¯¼ë²•, í˜•ë²• ë“±)ì˜ "
                "ì¡°í•­ì„ 30ì¤„ ì´ë‚´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”. ì„¤ëª… ì—†ì´ ë²•ë¥  ì¡°í•­ë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
            },
            {"role": "user", "content": "ì•„ë¬´ ë²•ì¡°í•­ì´ë‚˜ 30ì¤„ ì´ë‚´ë¡œ ì•Œë ¤ì¤˜!"},
        ]

        context = call_llm(messages, temperature=1.0)
        contexts.append(context)

    print(f"\nâœ… ì™„ë£Œ: {len(contexts)}ê°œ ìƒì„±")
    return contexts


def generate_similar_domain(base_context: str) -> str:
    """ìœ ì‚¬ ë„ë©”ì¸ ë…¸ì´ì¦ˆ ìƒì„±"""
    messages = [
        {
            "role": "system",
            "content": "ê¸ˆìœµ ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ, ì£¼ì–´ì§„ ë³¸ë¬¸ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì§ˆë¬¸ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” "
            "ê¸ˆìœµ ë²•ë¥  ì¡°í•­ì„ 30ì¤„ ì´ë‚´ë¡œ ìƒì„±í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´ ì¡°í•­ë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
        },
        {
            "role": "user",
            "content": f"ë‹¤ìŒ ë³¸ë¬¸ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ë¬´ê´€í•œ ê¸ˆìœµ ë²•ë¥  ì¡°í•­ì„ ìƒì„±í•˜ì„¸ìš”:\n\n{base_context}",
        },
    ]
    return call_llm(messages, temperature=0.8)


def generate_contradictory(base_context: str, answer: str) -> str:
    """ëª¨ìˆœ ì •ë³´ ìƒì„±"""
    messages = [
        {
            "role": "system",
            "content": "ê¸ˆìœµ ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ, ì£¼ì–´ì§„ ì •ë‹µê³¼ ëª¨ìˆœë˜ì§€ë§Œ ê·¸ëŸ´ë“¯í•œ ë²•ë¥  ì¡°í•­ì„ "
            "30ì¤„ ì´ë‚´ë¡œ ìƒì„±í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´ ì¡°í•­ë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
        },
        {
            "role": "user",
            "content": f"ë³¸ë¬¸: {base_context}\n\nì •ë‹µ: {answer}\n\nì •ë‹µê³¼ ëª¨ìˆœë˜ëŠ” ë²•ë¥  ì¡°í•­ì„ ìƒì„±í•˜ì„¸ìš”.",
        },
    ]
    return call_llm(messages, temperature=0.8)


def generate_partial_overlap(base_context: str) -> str:
    """ë¶€ë¶„ ê²¹ì¹¨ ì •ë³´ ìƒì„±"""
    messages = [
        {
            "role": "system",
            "content": "ê¸ˆìœµ ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ, ì£¼ì–´ì§„ ë³¸ë¬¸ê³¼ ì¼ë¶€ í‚¤ì›Œë“œëŠ” ê²¹ì¹˜ì§€ë§Œ ì˜ë¯¸ëŠ” ë‹¤ë¥¸ "
            "ë²•ë¥  ì¡°í•­ì„ 30ì¤„ ì´ë‚´ë¡œ ìƒì„±í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´ ì¡°í•­ë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
        },
        {
            "role": "user",
            "content": f"ë‹¤ìŒ ë³¸ë¬¸ê³¼ ì¼ë¶€ ê²¹ì¹˜ì§€ë§Œ ë‹¤ë¥¸ ë‚´ìš©ì˜ ë²•ë¥  ì¡°í•­ì„ ìƒì„±í•˜ì„¸ìš”:\n\n{base_context}",
        },
    ]
    return call_llm(messages, temperature=0.8)


def translate_to_english(text: str) -> str:
    """í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
    messages = [
        {
            "role": "system",
            "content": "ê¸ˆìœµ ë²•ë¥  ì „ë¬¸ ë²ˆì—­ê°€ë¡œì„œ, í•œêµ­ì–´ ë²• ì¡°í•­ì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. "
            "ì„¤ëª… ì—†ì´ ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
        },
        {"role": "user", "content": text},
    ]
    return call_llm(messages, temperature=0)


# ==================== ë…¸ì´ì¦ˆ ë°°ì¹˜ ====================


class NoisePositioner:
    """ë…¸ì´ì¦ˆ ìœ„ì¹˜ ì¡°ì •"""

    @staticmethod
    def apply(base: str, noises: List[str], position: str) -> str:
        """ë…¸ì´ì¦ˆë¥¼ ì§€ì •ëœ ìœ„ì¹˜ì— ë°°ì¹˜"""
        if not noises:
            return base

        combined = "\n\n".join(noises)

        if position == "prepend":
            return f"{combined}\n\n{base}"

        elif position == "append":
            return f"{base}\n\n{combined}"

        elif position == "sandwich":
            if len(noises) >= 2:
                return f"{noises[0]}\n\n{base}\n\n{noises[1]}"
            return f"{combined}\n\n{base}\n\n{combined}"

        elif position == "interleave":
            return NoisePositioner._interleave(base, noises)

        elif position == "random":
            return NoisePositioner._random_insert(base, combined)

        return base

    @staticmethod
    def _interleave(base: str, noises: List[str]) -> str:
        """ë¬¸ì¥ ì‚¬ì´ì‚¬ì´ì— ë…¸ì´ì¦ˆ ì‚½ì…"""
        base_sentences = [s.strip() for s in base.split(".") if s.strip()]
        noise_sentences = []
        for noise in noises:
            noise_sentences.extend([s.strip() for s in noise.split(".") if s.strip()])

        result = []
        noise_idx = 0

        for i, sentence in enumerate(base_sentences):
            result.append(sentence)
            if i % 2 == 1 and noise_idx < len(noise_sentences):
                result.append(noise_sentences[noise_idx])
                noise_idx += 1

        return ". ".join(result) + "."

    @staticmethod
    def _random_insert(base: str, noise: str, num_insertions: int = 3) -> str:
        """ë¬´ì‘ìœ„ ìœ„ì¹˜ì— ë…¸ì´ì¦ˆ ì‚½ì…"""
        base_sentences = [s.strip() for s in base.split(".") if s.strip()]
        noise_sentences = [s.strip() for s in noise.split(".") if s.strip()]

        for _ in range(min(num_insertions, len(noise_sentences))):
            if noise_sentences:
                pos = random.randint(0, len(base_sentences))
                base_sentences.insert(pos, random.choice(noise_sentences))

        return ". ".join(base_sentences) + "."


# ==================== ë©”ì¸ íŒŒì´í”„ë¼ì¸ ====================


class InferenceBenchmarkPipeline:
    """LLM ì¶”ë¡  ëŠ¥ë ¥ í‰ê°€ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config: PipelineConfig):
        self.config = config
        self.positioner = NoisePositioner()

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_path = Path(config.output_dir)
        self.output_path.mkdir(parents=True, exist_ok=True)
        self.timestamp = pd.Timestamp.now(tz="Asia/Seoul").strftime("%Y%m%d_%H%M%S")

        # ì‹œë“œ ì„¤ì •
        random.seed(config.random_seed)
        np.random.seed(config.random_seed)

    def _load_or_create(self, filename: str, create_func, *args) -> pd.DataFrame:
        """íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±"""
        filepath = self.output_path / filename

        try:
            df = pd.read_csv(filepath, index_col="id")
            print(f"âœ… íŒŒì¼ì—ì„œ ë¡œë“œ: {filename}")
            return df
        except FileNotFoundError:
            print(f"ğŸ“ ìƒˆë¡œ ìƒì„±: {filename}")
            data = create_func(*args)
            df = pd.DataFrame({"context": data})
            df.to_csv(filepath, index_label="id")
            return df

    def prepare_english_contexts(self, base_df: pd.DataFrame) -> pd.DataFrame:
        """ì˜ë¬¸ ë²ˆì—­ ë° ì…”í”Œ"""
        print("\n" + "=" * 80)
        print("ğŸ“– ì˜ë¬¸ ë²ˆì—­ ë° ì…”í”Œ")
        print("=" * 80)

        def create_english():
            contexts = []
            total = len(base_df)
            for idx, row in base_df.iterrows():
                print(f"  ë²ˆì—­ ì¤‘: {idx + 1}/{total}", end="\r")
                translated = translate_to_english(row["context"])
                contexts.append(translated)
            print(f"\nâœ… ë²ˆì—­ ì™„ë£Œ: {total}ê°œ")
            return contexts

        df = self._load_or_create("shuffled_english_contexts.csv", create_english)

        # ì…”í”Œ (ì´ë¯¸ ì €ì¥ëœ ê²½ìš° ë‹¤ì‹œ ì…”í”Œí•˜ì§€ ì•ŠìŒ)
        if len(df) == len(base_df):
            df = df.sample(frac=1, random_state=self.config.random_seed).reset_index(
                drop=True
            )
            df.to_csv(
                self.output_path / "shuffled_english_contexts.csv", index_label="id"
            )

        return df

    def prepare_noise_contexts(self, base_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """ëª¨ë“  ë…¸ì´ì¦ˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        print("\n" + "=" * 80)
        print("ğŸ”Š ë…¸ì´ì¦ˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±")
        print("=" * 80)

        noise_dfs = {}
        sample_size = 20

        # 1. ë¬´ê´€í•œ ë²•ë¥ 
        noise_dfs["unrelated"] = self._load_or_create(
            "unrelated_contexts.csv",
            generate_unrelated_contexts,
            self.config.num_unrelated_contexts,
        )

        # 2. ìœ ì‚¬ ë„ë©”ì¸
        def create_similar():
            contexts = []
            for idx, row in base_df.head(sample_size).iterrows():
                print(f"  ìœ ì‚¬ ë„ë©”ì¸ ìƒì„±: {idx + 1}/{sample_size}", end="\r")
                contexts.append(generate_similar_domain(row["context"]))
            print(f"\nâœ… ìœ ì‚¬ ë„ë©”ì¸ ì™„ë£Œ: {len(contexts)}ê°œ")
            return contexts

        noise_dfs["similar"] = self._load_or_create(
            "similar_domain_contexts.csv", create_similar
        )

        # 3. ëª¨ìˆœ ì •ë³´
        def create_contradictory():
            contexts = []
            for idx, row in base_df.head(sample_size).iterrows():
                print(f"  ëª¨ìˆœ ì •ë³´ ìƒì„±: {idx + 1}/{sample_size}", end="\r")
                contexts.append(generate_contradictory(row["context"], row["answer"]))
            print(f"\nâœ… ëª¨ìˆœ ì •ë³´ ì™„ë£Œ: {len(contexts)}ê°œ")
            return contexts

        noise_dfs["contradictory"] = self._load_or_create(
            "contradictory_contexts.csv", create_contradictory
        )

        # 4. ë¶€ë¶„ ê²¹ì¹¨
        def create_partial():
            contexts = []
            for idx, row in base_df.head(sample_size).iterrows():
                print(f"  ë¶€ë¶„ ê²¹ì¹¨ ìƒì„±: {idx + 1}/{sample_size}", end="\r")
                contexts.append(generate_partial_overlap(row["context"]))
            print(f"\nâœ… ë¶€ë¶„ ê²¹ì¹¨ ì™„ë£Œ: {len(contexts)}ê°œ")
            return contexts

        noise_dfs["partial"] = self._load_or_create(
            "partial_overlap_contexts.csv", create_partial
        )

        return noise_dfs

    def create_noisy_datasets(
        self,
        base_df: pd.DataFrame,
        english_df: pd.DataFrame,
        noise_dfs: Dict[str, pd.DataFrame],
    ) -> List[pd.DataFrame]:
        """ë…¸ì´ì¦ˆ ì¡°í•© ë°ì´í„°ì…‹ ìƒì„±"""
        print("\n" + "=" * 80)
        print("ğŸ² ë…¸ì´ì¦ˆ ì¡°í•© ë°ì´í„°ì…‹ ìƒì„±")
        print("=" * 80)

        datasets = []

        for difficulty_name, difficulty_config in self.config.difficulties.items():
            print(f"\nğŸ“Š ë‚œì´ë„: {difficulty_name.upper()}")
            print(f"   {difficulty_config['description']}")

            for position in difficulty_config["positions"]:
                print(f"   ìœ„ì¹˜: {position}")

                rows = []

                for idx, row in base_df.iterrows():
                    # ë…¸ì´ì¦ˆ ì„ íƒ
                    noises = []
                    for noise_type in difficulty_config["noise_types"]:
                        if noise_type == "english":
                            if idx < len(english_df):
                                noises.append(english_df.iloc[idx]["context"])
                        elif noise_type in noise_dfs:
                            noise_idx = random.randint(
                                0, len(noise_dfs[noise_type]) - 1
                            )
                            noises.append(
                                noise_dfs[noise_type].iloc[noise_idx]["context"]
                            )

                    # ë…¸ì´ì¦ˆ ë°°ì¹˜
                    noisy_context = self.positioner.apply(
                        row["context"], noises, position
                    )

                    rows.append(
                        {
                            "id": idx,
                            "difficulty": difficulty_name,
                            "position": position,
                            "noise_types": ",".join(difficulty_config["noise_types"]),
                            "original_context": row["context"],
                            "noisy_context": noisy_context,
                            "question": row["question"],
                            "answer": row["answer"],
                            "reasoning": row.get("reasoning", row.get("reason", "")),
                        }
                    )

                df = pd.DataFrame(rows)
                datasets.append(df)

                # ì €ì¥
                filename = f"noisy_{difficulty_name}_{position}_{self.timestamp}.csv"
                df.to_csv(self.output_path / filename, index=False)
                print(f"   âœ… ì €ì¥: {filename}")

        return datasets

    def create_summary(self, datasets: List[pd.DataFrame]) -> pd.DataFrame:
        """í‰ê°€ ìš”ì•½ ìƒì„±"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ í‰ê°€ ìš”ì•½ ìƒì„±")
        print("=" * 80)

        rows = []
        for df in datasets:
            if len(df) == 0:
                continue

            rows.append(
                {
                    "difficulty": df.iloc[0]["difficulty"],
                    "position": df.iloc[0]["position"],
                    "num_samples": len(df),
                    "noise_types": df.iloc[0]["noise_types"],
                    "avg_noisy_length": df["noisy_context"].str.len().mean(),
                    "avg_original_length": df["original_context"].str.len().mean(),
                    "noise_ratio": df["noisy_context"].str.len().mean()
                    / df["original_context"].str.len().mean(),
                }
            )

        summary_df = pd.DataFrame(rows)
        filename = f"summary_{self.timestamp}.csv"
        summary_df.to_csv(self.output_path / filename, index=False)
        print(f"âœ… ìš”ì•½ ì €ì¥: {filename}")

        return summary_df

    def run(self) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("\n" + "=" * 80)
        print("ğŸš€ LLM ì¶”ë¡  ëŠ¥ë ¥ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print(f"â° ì‹œì‘: {pd.Timestamp.now(tz='Asia/Seoul')}")
        print("=" * 80)

        # 1. ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ
        print(f"\nğŸ“‚ ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ: {self.config.base_context_file}")
        base_df = pd.read_csv(self.config.base_context_file)
        if "id" not in base_df.columns:
            base_df = base_df.reset_index().rename(columns={"index": "id"})
        print(f"âœ… {len(base_df)}ê°œ ìƒ˜í”Œ ë¡œë“œ")

        # 2. ì˜ë¬¸ ë²ˆì—­
        english_df = self.prepare_english_contexts(base_df)

        # 3. ë…¸ì´ì¦ˆ ìƒì„±
        noise_dfs = self.prepare_noise_contexts(base_df)

        # 4. ë…¸ì´ì¦ˆ ë°ì´í„°ì…‹ ìƒì„±
        datasets = self.create_noisy_datasets(base_df, english_df, noise_dfs)

        # 5. ìš”ì•½
        summary_df = self.create_summary(datasets)

        print("\n" + "=" * 80)
        print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"â° ì¢…ë£Œ: {pd.Timestamp.now(tz='Asia/Seoul')}")
        print(f"ğŸ“ ì¶œë ¥: {self.output_path}")
        print("=" * 80)

        return {
            "base_df": base_df,
            "english_df": english_df,
            "noise_dfs": noise_dfs,
            "datasets": datasets,
            "summary_df": summary_df,
        }


# ==================== ì‹¤í–‰ ====================

if __name__ == "__main__":
    config = PipelineConfig(
        base_context_file="./dataset/base_contexts.csv",
        output_dir="./output",
        num_unrelated_contexts=70,
        random_seed=42,
    )

    pipeline = InferenceBenchmarkPipeline(config)
    results = pipeline.run()

    print("\n" + "=" * 80)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    print(f"ìƒì„±ëœ ë°ì´í„°ì…‹: {len(results['datasets'])}ê°œ")
    print(f"\ní‰ê°€ ìš”ì•½:\n{results['summary_df'].to_string()}")
